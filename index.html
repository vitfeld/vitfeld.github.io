<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Vitaly Feldman's personal homepage</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <!-- Custom styles for this template -->
    <link href="cover.css" rel="stylesheet">
  </head>

  <body>

    <div class="site-wrapper">

      <div class="site-wrapper-inner">

        <div class="cover-container">

          <div class="masthead clearfix">
            <div class="inner">
              <h3 class="masthead-brand">Vitaly Feldman</h3>
              <nav class="nav nav-masthead">
                <a class="nav-link active" href="#">Home</a>
                <a class="nav-link" href="papers.html">Publications</a>
                <a class="nav-link" href="bio.html">Bio</a>
              </nav>
            </div>
          </div>

          <div class="inner cover">
          <p class="lead">
            <img src="vitaly.jpg" class="img-fluid float-left padright" height="300" width="240"/>
            I&#39;m a research scientist at Apple AI Research</a>.

            </br>
            <div class="container-fluid">
          <p> I work on theoretical aspects of machine learning. Recent topics include <a href="http://wadapt.net/">adaptive data analysis</a>, complexity of learning with constrained access to data, and privacy-preserving learning. I also worked on understanding of natural learning systems: learning by the brain and evolution as learning. </p
          </div>
          </div>

          <div class="mypage">

          <h5>
         <a href="#recentactivities" data-toggle="collapse" >Academic activities</a> </h5>
          <div id="recentactivities" class="collapse">
          <p> I serve as a director and treasurer on the steering committee of the <a href="http://www.learningtheory.org/acl/">Association for Computational Learning</a></p>
          <ul>
<li>  Co-organizer of  <a href="https://simons.berkeley.edu/privacy2019-2">Privacy and the Science of Data Analysis</a> workshop at the Simons Institute for the Theory of Computing, UC Berkeley. Apr 8-12, 2019 (with  Kamalika Chaudhuri, Sesa Slavkovic, Anand Sarwate and Adam Smith).
<li>  Co-organizer of  <a href="https://simons.berkeley.edu/events/field-trip-google">Google-Simons Institute</a> workshop on privacy. Apr. 5, 2019 (with Ravi Kumar, Ilya Mironov and Adam Smith).
<li>  Co-organizer of  <a href="https://simons.berkeley.edu/privacy2019">Data Privacy: Foundations and Applications</a> program at the Simons Institute for the Theory of Computing, UC Berkeley. Jan-May 2019 (with Katrina Ligett, Kobbi Nissim, Sesa Slavkovic and Adam Smith).
<li>            July 2017: visiting <a href="https://www.cwi.nl/">Centrum Wiskunde & Informatica (CWI)</a>, Amsterdam (hosted by Peter Grunwald).
<li>            May 2017: Co-organizer of <a href="https://simons.berkeley.edu/workshops/privacy2017"> Data Privacy: Planning Workshop</a> at the Simons Institute.
<li>            March-May 2017: visiting scientist at the <a href="https://simons.berkeley.edu/programs/machinelearning2017">Simons Institute. Foundations of Machine Learning program.</a>
<li>            Dec 2016: Co-organizer of workshop on Adaptive Data Analysis <a href="http://www.wadapt.net/">(WADAPT)</a> at <a href="https://nips.cc/Conferences/2016">NIPS 2016</a>  (with Aaditya Ramdas, Aaron Roth and Adam Smith).
</ul>

<b> Recent program committees:</b>
 <ul>
<li>   <a href="http://www.learningtheory.org/colt2016/">COLT 2016</a>: Co-chair with <a href="http://www.mit.edu/~rakhlin/">Sasha Rakhlin</a>.
<li>   <a href="http://algorithmiclearningtheory.org/">ALT 2021</a>: Co-chair with <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>.
<li> <a href=""http://itw2020.it/welcome.html">ITW 2020</a>, <a href="http://www.learningtheory.org/colt2020/">COLT 2020</a> Senior PC and Open Problems chair, <a href="https://icml.cc/Conferences/2020">ICML 2020</a> (AC), <a href="http://nips.cc/Conferences/2019/">NeurIPS 2019</a> (AC), <a href="http://www.learningtheory.org/colt2019/">COLT 2019</a>, <a href="https://icml.cc/Conferences/2019">ICML 2019</a> (AC), <a href="http://alt2019.algorithmiclearningtheory.org/">ALT 2019</a>, <a href="http://nips.cc/Conferences/2018/">NIPS 2018</a> (AC), <a href="http://www.learningtheory.org/colt2018/">COLT 2018</a>, <a href="http://acm-stoc.org/">STOC 2018</a>.
</ul>
        </div>

          <h5>
         <a href="#recentpapers" data-toggle="collapse">Recent/selected works</a>  </h5>
          <div id="recentpapers" class="collapse">
           <ol>
          <li> <a href="https://arxiv.org/abs/2008.11193">Individual Privacy Accounting via a Renyi Filter.</a>
          <br>
           With Tijana Zrnic. <i> Aug, 2020 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/2008.03703">What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation.</a>
          <br>
           With Chiyuan Zhang. <i> June, 2020 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1911.04014">Interaction is necessary for distributed learning with privacy or communication constraints.</a>
          <br>
           With Yuval Dagan. <i> STOC 2020 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/2005.04763">Private Stochastic Convex Optimization: Optimal Rates in Linear Time.</a>
          <br>
           With Tomer Koren and Kunal Talwar. <i> STOC 2020 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1911.10541">PAC learning with stable and private predictions .</a>
          <br>
           With Yuval Dagan. <i> COLT 2020 </i>.
          </li>

          <li> <a href="http://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail.</a>
          <br>
           <i> STOC 2020 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1908.09970">Private Stochastic Convex Optimization with Optimal Rates.</a>
          <br>
           With Raef Bassily, Kunal Talwar and Abhradeep Thakurta.<i> NeurIPS 2019 <b>(spotlight)</b>. </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1902.10710">High probability generalization bounds for uniformly stable algorithms with nearly optimal rate.</a>
          <br>
           With Jan Vondrak. <i> COLT 2019 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1905.10360">The advantages of multiple classes for reducing overfitting from test set reuse.</a>
          <br>
           With Roy Frostig and Moritz Hardt. <i> ICML 2019 </i>.
          </li>
          <li> <a href="https://arxiv.org/abs/1809.09165">Locally Private Learning without Interaction Requires Separation</a>.
          <br>
           With Amit Daniely. <i> NeurIPS 2019. </i>
          </li>
          <li> <a href="https://arxiv.org/abs/1811.12469">Amplification by Shuffling: From Local to Central Differential Privacy</a>.
          <br>
           With Ulfar Erlingsson, Ilya Mironov, Ananth Raghunathan,  Kunal Talwar, Abhradeep Thakurta. <i> SODA 2019.</i>
          </li>
          <li> <a href="https://arxiv.org/abs/1812.09859">Generalization Bounds for Uniformly Stable Algorithms.</a>
          <br>
           With Jan Vondrak. <i> NIPS 2018 <b>(spotlight)</b>. </i>
          </li>
          <li> <a href="https://arxiv.org/abs/1808.06651">Privacy Amplification by Iteration. </a>
          <br>
           With Ilya Mironov, Kunal Talwar and Abhradeep Thakurta. <i> FOCS 2018 </i>.
          </li>
          <li> <a href="https://arxiv.org/abs/1803.10266">Privacy-preserving Prediction. </a>
          <br>
           With Cynthia Dwork. <i> COLT 2018 </i>.
          </li>
            <li> <a href="https://arxiv.org/abs/1712.07196">Calibrating Noise to Variance in Adaptive Data Analysis. </a>
          <br>
           With Thomas Steinke. <i> COLT 2018 </i>.
          </li>
          <li> <a href="http://arxiv.org/abs/1608.02198">A General Characterization of the Statistical Query Complexity</a>.
          <br>
          <i> COLT 2017 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1608.04414">Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back</a>.
          <br>
          <i> NIPS 2016 <b>(oral presentation)</b>. </i>
          </li>

          <li> <a href="papers.html#DFH_Science">The reusable holdout: Preserving validity in adaptive data analysis</a>.
          <br>
          With  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold and Aaron Roth. <i><b>Science</b>, 2015.</i>
          <br> <a href="http://researcher.watson.ibm.com/researcher/view_group.php?id=5855"><b>IBM Research 2015 Best Paper Award</b></a>.
          <br> Based on STOC and NIPS papers below. See also my post on this work at <a href="http://www.kdnuggets.com/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html">IBM Research blog (republished by KDnuggets)</a>.

          </li>

          <li> <a href="https://arxiv.org/abs/1506.02629">Generalization in Adaptive Data Analysis and Holdout Reuse</a>.
          <br>
          With Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold and Aaron Roth. <i> NIPS, 2015</i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1411.2664">Preserving Statistical Validity in Adaptive Data Analysis</a>.
          <br>
          With  Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold and Aaron Roth. <i> STOC 2015.</i>
          <br>
          Invited to <b> SICOMP special issue on STOC</b>
          </li>

          <li> <a href="https://arxiv.org/abs/1311.4821"> On the Complexity of Random Satisfiability Problems with Planted Solutions </a>.
          <br>
          With Will Perkins and Santosh Vempala. <i>STOC 2015 </i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1402.6278">Sample Complexity Bounds on Differentially Private Learning via Communication Complexity
           </a>.
          <br>
          With David Xiao. <i> COLT 2014</i>, <i>SICOMP 2015</i>.
          </li>

          <li> <a href="https://arxiv.org/abs/1307.3301">Optimal Bounds on Approximation of Submodular and XOS Functions by Juntas</a>.
          <br>
          With Jan Vondrak. <i> FOCS 2013 </i>. <i>SICOMP 2016, <b> Special issue on FOCS</b></i>
          <br> <a href="http://researcher.watson.ibm.com/researcher/view_group.php?id=5855"><b>IBM Research 2016 Best Paper Award</b></a>.
          </li>

          <li> <a href="https://arxiv.org/abs/1211.0996">Learning using Local Membership Queries</a>.
          <br>
          With Pranjal Awasthi and Varun Kanade. <i> COLT 2013, <b>Best Student (co-authored) Paper Award</b> </i>
          </li>

          <li> <a href="https://arxiv.org/abs/1201.1214">Statistical Algorithms and a Lower Bound for Detecting Planted Cliques</a>.
          <br>
          With Elena Grigorescu, Lev Reyzin, Santosh Vempala and Ying Xiao. <i> STOC 2013 </i>. <b>JACM</b> 2017 .
          </li>

          <li> <a href="https://arxiv.org/abs/1206.0985">Nearly Optimal Solutions for the Chow Parameters Problem and Low-weight Approximation of Halfspaces</a>.
          <br>
          With Anindya De, Ilias Diakonikolas and Rocco Servedio. <i>STOC 2012; <b>JACM</b> 2014 </i>.
          <br> <a href="http://researcher.watson.ibm.com/researcher/view_group.php?id=5855"><b>IBM Research 2014 Best Paper Award</b></a>.
          </li>

          <li><a href="https://arxiv.org/abs/0909.2927">Distribution-Specific Agnostic Boosting</a>.
          <br>
          <i>ITCS (formerly ICS) 2010.</i></li>


          <li><a href="https://arxiv.org/abs/1002.3183">A Complete Characterization of Statistical Query Learning with Applications to Evolvability</a>.
          <br>
           <i>FOCS 2009; JCSS 2012 <b>(Special issue on Learning Theory)</b>.</i></li>

          <li><a href="papers.html#FV09_NeuroModel">Experience-Induced Neural Circuits That Achieve High Capacity.</a>.
          <br>
          With Leslie Valiant.<i> <a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.08-08-851">Neural Computation 21:10, 2009</a>.</i></li>


          <li><a href="papers.html#FGKP06_AgnParHalf">New Results for Learning Noisy Parities and Halfspaces</a>.
          <br>
          With Parikshit Gopalan, Subhash Khot, and Ashok Ponnuswami. <i>FOCS 2006; SICOMP 2009, <b>Special issue on FOCS</b></i></li>

          <li><a href="papers.html#F06_MinDNF">Hardness of Approximate Two-level Logic Minimization and PAC Learning with Membership Queries</a>.
          <br>
           <i>STOC 2006; JCSS 75(1), 2009 <b>(Special issue on Learning Theory)</b></i></li>

          <li><a href="papers.html#F05_AENADNF">Attribute Efficient and Non-adaptive Learning of Parities and DNF Expressions</a>.
          <br>
           <i>COLT 2005, <b>Best Student Paper Award</b>; <a href="http://jmlr.csail.mit.edu/papers/v8/feldman07a.html">JMLR 2007</a>, <b>Special issue on COLT</b></i></li>

          <li><a href="papers.html#ABFKP04_ProperHardness">The Complexity of Properly Learning Simple Concept Classes</a>.<br>
          With Misha Alekhnovich, Mark Braverman, Adam Klivans, and Toni Pitassi.<br>
           <i>FOCS 2004; JCSS 74(1), 2008 <b>(Special issue on Learning Theory)</b></i></li>
          </ol>

          </div>

          <h5>
         <a href="#recenttalks" data-toggle="collapse">Slides/recordings for some recent talks</a> </h5>
          <div id="recenttalks" class="collapse">
          <ul>
              <li> Does Learning Require Memorization? A Short Tale about a Long Tail. Princeton/Harvard/MIT/TAU/ATI/DeepMind/Google (Sep 2019+):<a href="slides/Memorization-long-Google.pptx">slides</a>,  long talk <a href="https://youtu.be/Fp7cgHRl8Yc">video</a> courtesy of Alan Turing Institute and a <a href="https://www.youtube.com/watch?v=sV59uoWJRnk">shorter version</a> recorded for STOC.
              <li> High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. COLT 2019:<a href="slides/Feldman-Unistab.COLT2019.pptx">slides</a>, <a href="https://www.youtube.com/watch?v=EBsUgAqH6u">video</a>
              <li> Locally Private Learning without Interaction Requires Separation. <a href="https://simons.berkeley.edu/privacy2019-2">Privacy and the Science of Data Analysis </a> workshop (Apr 2019):<a href="slides/Adaptive-LPD-simons.pptx">slides</a>.
              <li> Amplification by Shuffling: From Local to Central Differential Privacy. <a href="https://ppml-workshop.github.io/ppml/">Privacy-preserving Machine Learning </a> workshop (Dec 2018); <a href="https://ita.ucsd.edu/ws/">ITA 2019</a>, Simons Institute seminar (Feb 2019):<a href="slides/L2C-Simons.pptx">slides</a>.
              <li> Privacy-preserving prediction. COLT 2018; <a href="https://tripods.soe.ucsc.edu/program-data-privacy-workshop">Privacy in Graphs workshop</a> (Nov 2018); <a href="https://ww2.amstat.org/meetings/jsm/2019/">JSM</a> 2019. <a href="slides/DPAPI-SC-workshop.pptx">slides</a>.
              <li> Generalization bounds for uniformly stable algorithms. <a href="https://simons.berkeley.edu/data-science-2018-2">Robust and High-Dimensional Statistics workshop</a> (Oct 2018):<a href="slides/UniStab-RobustSimons.pptx">slides</a>,  <a href="https://youtu.be/cQlDAMeAyjc">video</a>. NIPS spotlight <a href="https://www.facebook.com/nipsfoundation/videos/299014817615185/"> video</a>.
              <li> Stability, Information and Generalization in Adaptive Data Analysis. Google NYC/Princeton/Penn (Apr. 2018): <a href="slides/Calibrate-Penn.pptx">slides</a>.
              <li> Dealing with Range Anxiety in Mean Estimation. ALT 2017 (Nov 2017): <a href="slides/SQ-range.ALT.pptx">slides</a>.
              <li> A General Characterization of the Statistical Query Complexity. COLT 2017 (July 2017); NYU (Feb. 2018): <a href="slides/SQ-DIM.NYU.pptx">slides</a>.
              <li> Understanding Generalization in Adaptive Data Analysis <a href="https://simons.berkeley.edu/workshops/machinelearning2017-3">Computational Challenges in Machine Learning workshop</a>, <a href="https://suri.epfl.ch/">EPFL</a>, and <a href="http://people.seas.harvard.edu/~yaron/datadriven2017/">Bertinoro</a> (2017):<a href="slides/AdaptiveGen.EPFL.pptx">slides</a>,  <a href="https://simons.berkeley.edu/talks/vitaly-feldman-2017-5-2">video</a>.
              <li> On the power of learning from <i>k</i>-wise queries. <a href="http://itcs-conf.org/">ITCS 2017</a>: <a href="slides/k-wise-VitalyF.pptx">slides</a>, <a href="https://www.youtube.com/watch?v=AQzIlSvTd8U"> video</a>.
              <li> Lower bounds against convex relaxations via the statistical query complexity. Caltech/UCLA/Stanford/Harvard/MIT, 2017: <a href="slides/SQ-alg-lb-VitalyF.pptx">slides</a> (with some comments in the notes).
              <li> Generalization of ERM in stochastic convex optimization. <a href="https://nips.cc">NIPS 2016</a>: <a href="slides/ERM-SCO-NIPS-VitalyF.pptx">slides</a> and <a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generalization-of-ERM-in-Stochastic-Convex-Optimization-The-Dimension-Strikes-Back">video</a>
              <li> Generalization and adaptivity in stochastic convex optimization. <a href="https://www.eventbrite.com/e/toca-sv-inaugural-meeting-tickets-28565107004">TOCA-SV 2016</a>: <a href="slides/ERM-SCO-Vitaly.pptx">slides</a> (with some comments in the notes).
              <li> Generalization in Adaptive Data Analysis via Max-Information. Simons Institute workshop on Information Theory, 2016: <a href="slides/MaxInfo-IT-reunion.pptx">slides</a>.
              <li> Preserving Validity in Adaptive Data Analysis. National Academy of Engineering, 2016: <a href="slides/Adaptive-JAFOE.pptx">slides</a>.
              <li> Adaptive Data Analysis without Overfitting.  Workshop on Learning. NUS, 2015: <a href="slides/HoldoutReuse-LearningW-NUS.pptx">slides</a>.
              <li> Preserving statistical validity in adaptive data analysis. STOC 2015: <a href="slides/AdaptiveSQ-STOC15.pptx">slides</a>.
              <li> Approximate resilience, monotonicity, and the complexity of agnostic learning. SODA 2015: <a href="slides/AppResilience-SODA15.pptx">slides</a>.
              <li> Sample complexity bounds on differentially private learning via communication complexity. COLT 2014; ITA 2015: <a href="slides/PrivateSample-ITA15.pptx">slides</a>.
              <li> Using data privacy for better adaptive predictions. Foundations of Learning Theory workshop @ COLT 2014 : <a href="slides/AdaptiveSQ-FLT14.pptx">slides</a>.
              <li> On the power and the limits of evolvability. Simons Institute workshop on Computational Theories of Evolution, 2014: <a href="slides/EvolveFromLearn.pptx">slides</a>.
            <li> Optimal bounds on approximation of submodular and XOS functions by juntas. Simons Institute workshop on Real Analysis at @FOCS 2013 : <a href="slides/SubmodJunta-Simons2013.pptx">slides</a>.
        </ul>
         </div>


        </div>
          <div class="mastfoot">
            <div class="inner">
              <p>Contact: firstname.edu@gmail.com </p>
            </div>
          </div>

        </div>

      </div>

    </div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"><\/script>')</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
  </body>
</html>
